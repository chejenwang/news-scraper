"""
TASS æ–°èçˆ¬èŸ² - æ”¹é€²ç‰ˆï¼ˆé›™ç¿»è­¯å¼•æ“ï¼‰
æ”¯æ´ googletrans å’Œ deep-translator é›™å¼•æ“
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import random
from typing import List, Dict
import re
from openpyxl import load_workbook
from openpyxl.styles import Alignment

class TASSNewsScraper:
    def __init__(self, headless=True):
        """åˆå§‹åŒ–çˆ¬èŸ²ï¼Œä½¿ç”¨ Selenium"""
        print("æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨...")
        
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument('--headless')  # ç„¡é ­æ¨¡å¼
        
        # éš±è—è­¦å‘Šè¨Šæ¯
        chrome_options.add_argument('--log-level=3')
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
        
        # æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--lang=ru-RU')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # é¡åˆ¥åç¨±å°ç…§
        self.categories = {
            'politika': 'æ”¿æ²»',
            'ekonomika': 'ç¶“æ¿Ÿ',
            'armiya-i-opk': 'è»äº‹èˆ‡åœ‹é˜²',
            'mezhdunarodnaya-panorama': 'åœ‹éš›å…¨æ™¯'
        }
        
        print("âœ“ ç€è¦½å™¨å•Ÿå‹•å®Œæˆ\n")
    
    def __del__(self):
        """é—œé–‰ç€è¦½å™¨"""
        try:
            self.driver.quit()
            print("\nâœ“ ç€è¦½å™¨å·²é—œé–‰")
        except:
            pass
    
    def human_delay(self, min_seconds=3, max_seconds=7):
        """æ¨¡æ“¬äººé¡é–±è®€å»¶é²"""
        delay = random.uniform(min_seconds, max_seconds)
        print(f"  ç­‰å¾… {delay:.1f} ç§’...")
        time.sleep(delay)
    
    def scroll_page(self):
        """æ¨¡æ“¬äººé¡æ»¾å‹•é é¢"""
        total_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_pause = random.uniform(0.5, 1.5)
        
        # åˆ†æ®µæ»¾å‹•
        for i in range(0, total_height, 300):
            self.driver.execute_script(f"window.scrollTo(0, {i});")
            time.sleep(scroll_pause)
    
    def translate_to_chinese(self, text: str, max_retries=3) -> str:
        """ç¿»è­¯ä¿„æ–‡åˆ°ç¹é«”ä¸­æ–‡ - ä½¿ç”¨å¤šå€‹å‚™ç”¨ç¿»è­¯å¼•æ“"""
        if not text or len(text.strip()) == 0:
            return ""
        
        # æ–¹æ³•1: å„ªå…ˆä½¿ç”¨ googletrans (å…è²»ä¸”è¼ƒç©©å®š)
        for attempt in range(max_retries):
            try:
                from googletrans import Translator
                translator = Translator()
                
                max_length = 4500
                if len(text) > max_length:
                    chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
                    translated_chunks = []
                    for chunk in chunks:
                        result = translator.translate(chunk, src='ru', dest='zh-tw')
                        translated_chunks.append(result.text)
                        time.sleep(random.uniform(1.5, 2.5))
                    return ''.join(translated_chunks)
                else:
                    result = translator.translate(text, src='ru', dest='zh-tw')
                    return result.text
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = random.uniform(4, 7)
                    print(f"  googletrans å¤±æ•—ï¼Œç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦ ({attempt + 1}/{max_retries})...")
                    time.sleep(wait_time)
                else:
                    print(f"  googletrans å¤±æ•—: {str(e)[:50]}")
        
        # æ–¹æ³•2: å‚™ç”¨æ–¹æ¡ˆ - ä½¿ç”¨ deep-translator
        try:
            from deep_translator import GoogleTranslator
            print("  åˆ‡æ›åˆ° deep-translator...")
            
            max_length = 4500
            if len(text) > max_length:
                chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
                translated_chunks = []
                for chunk in chunks:
                    translator = GoogleTranslator(source='ru', target='zh-TW')
                    result = translator.translate(chunk)
                    translated_chunks.append(result)
                    time.sleep(random.uniform(2, 3))
                return ''.join(translated_chunks)
            else:
                translator = GoogleTranslator(source='ru', target='zh-TW')
                return translator.translate(text)
        except Exception as e:
            print(f"  deep-translator ä¹Ÿå¤±æ•—: {str(e)[:50]}")
        
        return "[ç¿»è­¯å¤±æ•—]"
    
    def get_article_links(self, category_url: str, max_articles: int = 10) -> List[str]:
        """ç²å–é¡åˆ¥é é¢ä¸­çš„æ–‡ç« é€£çµ"""
        print(f"\næ­£åœ¨ç²å–æ–‡ç« åˆ—è¡¨: {category_url}")
        
        try:
            self.driver.get(category_url)
            self.human_delay(3, 5)
            
            # æ»¾å‹•é é¢è¼‰å…¥æ›´å¤šå…§å®¹
            print("  æ»¾å‹•é é¢è¼‰å…¥å…§å®¹...")
            self.scroll_page()
            time.sleep(2)
            
            # ç²å–é é¢æºç¢¼
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            article_links = []
            news_links = soup.find_all('a', href=True)
            
            for a_tag in news_links:
                href = a_tag['href']
                
                # TASS æ–‡ç«  URL æ¨¡å¼
                if re.search(r'/(politika|ekonomika|armiya-i-opk|mezhdunarodnaya-panorama)/\d+', href):
                    # è½‰æ›ç‚ºå®Œæ•´ URL
                    if href.startswith('http'):
                        full_url = href
                    elif href.startswith('/'):
                        full_url = f"https://tass.ru{href}"
                    else:
                        continue
                    
                    # å»é‡ä¸¦ç¢ºä¿æ˜¯ tass.ru åŸŸå
                    if 'tass.ru' in full_url and full_url not in article_links:
                        article_links.append(full_url)
                        print(f"  æ‰¾åˆ°: {full_url}")
                        
                        if len(article_links) >= max_articles:
                            break
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦æ›´å¯¬é¬†çš„åŒ¹é…
            if len(article_links) == 0:
                print("  ä½¿ç”¨å‚™ç”¨æ–¹æ³•æœå°‹é€£çµ...")
                for a_tag in news_links:
                    href = a_tag.get('href', '')
                    if re.search(r'/\d{7,}', href) and \
                       not any(x in href for x in ['page', 'tag', 'search', 'about']):
                        
                        if href.startswith('/'):
                            full_url = f"https://tass.ru{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        
                        if 'tass.ru' in full_url and full_url not in article_links:
                            article_links.append(full_url)
                            print(f"  æ‰¾åˆ°: {full_url}")
                            
                            if len(article_links) >= max_articles:
                                break
            
            print(f"ç¸½å…±æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« \n")
            return article_links
            
        except Exception as e:
            print(f"âŒ ç²å–æ–‡ç« åˆ—è¡¨éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        text = text.replace('\xa0', ' ')
        return text
    
    def scrape_article(self, url: str) -> Dict:
        """çˆ¬å–å–®ç¯‡æ–‡ç« """
        print(f"\n{'='*70}")
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        
        try:
            self.driver.get(url)
            self.human_delay(4, 8)  # æ¯ç¯‡æ–‡ç« ç­‰å¾… 4-8 ç§’
            
            # æ»¾å‹•é é¢ç¢ºä¿æ‰€æœ‰å…§å®¹è¼‰å…¥
            self.scroll_page()
            time.sleep(1)
            
            # ç²å–é é¢æºç¢¼
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            article_data = {
                'url': url,
                'download_datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # === æ¨™é¡Œ ===
            title = ''
            try:
                title_elem = self.driver.find_element(By.TAG_NAME, 'h1')
                title = self.clean_text(title_elem.text)
            except:
                title_tag = soup.find('h1')
                if title_tag:
                    title = self.clean_text(title_tag.get_text())
            
            if not title:
                meta_title = soup.find('meta', {'property': 'og:title'})
                if meta_title:
                    title = meta_title.get('content', '')
            
            article_data['title_ru'] = title
            print(f"âœ“ æ¨™é¡Œ: {title[:60]}...")
            
            # === æ—¥æœŸ ===
            date = ''
            date_selectors = [
                (By.TAG_NAME, 'time'),
                (By.CLASS_NAME, 'date'),
                (By.CLASS_NAME, 'Datetime'),
            ]
            
            for by, value in date_selectors:
                try:
                    date_elem = self.driver.find_element(by, value)
                    date = self.clean_text(date_elem.text)
                    if not date:
                        date = date_elem.get_attribute('datetime') or ''
                    if date:
                        break
                except:
                    continue
            
            # å‚™ç”¨ï¼šå¾ soup æ‰¾
            if not date:
                time_tag = soup.find('time')
                if time_tag:
                    date = time_tag.get('datetime', '') or self.clean_text(time_tag.get_text())
            
            article_data['date'] = date
            print(f"âœ“ æ—¥æœŸ: {date if date else 'æœªæ‰¾åˆ°'}")
            
            # === ä½œè€… ===
            author = ''
            try:
                author_elem = self.driver.find_element(By.CLASS_NAME, 'Author')
                author = self.clean_text(author_elem.text)
            except:
                pass
            
            if not author:
                author_tag = soup.find('span', class_=re.compile(r'author', re.I))
                if author_tag:
                    author = self.clean_text(author_tag.get_text())
            
            if not author:
                author = 'TASS'
            
            article_data['author'] = author
            print(f"âœ“ ä½œè€…: {author}")
            
            # === å…§æ–‡ ===
            content_parts = []
            
            # å˜—è©¦æ‰¾åˆ°æ–‡ç« ä¸»é«”
            try:
                article_body = self.driver.find_element(By.CLASS_NAME, 'text-block')
                paragraphs = article_body.find_elements(By.TAG_NAME, 'p')
                
                for para in paragraphs:
                    text = self.clean_text(para.text)
                    if len(text) > 20:
                        content_parts.append(text)
            except:
                # å‚™ç”¨æ–¹æ¡ˆï¼šå¾ soup æå–
                print("  ä½¿ç”¨å‚™ç”¨æ–¹æ³•æå–å…§æ–‡...")
                
                article_containers = soup.find_all(['article', 'div'], class_=re.compile(r'text-block|article-text|content|body', re.I))
                
                for container in article_containers:
                    for para in container.find_all(['p', 'h2', 'h3']):
                        if para.find_parent(['aside', 'footer', 'nav', 'header']):
                            continue
                        
                        text = self.clean_text(para.get_text())
                        if len(text) > 20 and text not in content_parts:
                            content_parts.append(text)
                
                # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œç›´æ¥æ‰¾æ‰€æœ‰ p æ¨™ç±¤
                if len(content_parts) < 2:
                    for p in soup.find_all('p'):
                        if p.find_parent(['nav', 'footer', 'header', 'aside']):
                            continue
                        text = self.clean_text(p.get_text())
                        if len(text) > 30 and text not in content_parts:
                            content_parts.append(text)
            
            content = '\n\n'.join(content_parts)
            article_data['content_ru'] = content
            
            print(f"âœ“ å…§æ–‡é•·åº¦: {len(content)} å­—ç¬¦ï¼Œ{len(content_parts)} æ®µè½")
            
            # è­¦å‘Š
            if not content:
                print("âš ï¸ è­¦å‘Š: æœªèƒ½æå–åˆ°å…§æ–‡")
            if not date:
                print("âš ï¸ è­¦å‘Š: æœªèƒ½æå–åˆ°æ—¥æœŸ")
            
            return article_data
            
        except Exception as e:
            print(f"âŒ çˆ¬å–éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def scrape_category(self, category_url: str, max_articles: int = 10) -> List[Dict]:
        """çˆ¬å–å–®ä¸€é¡åˆ¥çš„æ–°è"""
        category_name = category_url.split('/')[-1]
        print(f"\n{'#'*70}")
        print(f"# é–‹å§‹çˆ¬å–é¡åˆ¥: {self.categories.get(category_name, category_name)}")
        print(f"# URL: {category_url}")
        print(f"{'#'*70}")
        
        # ç²å–æ–‡ç« é€£çµ
        article_links = self.get_article_links(category_url, max_articles)
        
        if not article_links:
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« é€£çµ")
            return []
        
        # çˆ¬å–æ–‡ç« 
        articles = []
        for i, link in enumerate(article_links, 1):
            print(f"\né€²åº¦: {i}/{len(article_links)}")
            
            article = self.scrape_article(link)
            if article:
                article['category'] = self.categories.get(category_name, category_name)
                articles.append(article)
            
            # æ¯ 3 ç¯‡ä¼‘æ¯ 10-15 ç§’
            if i % 3 == 0 and i < len(article_links):
                print("\nä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è¢«åµæ¸¬...")
                rest_time = random.uniform(10, 15)
                print(f"  ä¼‘æ¯ {rest_time:.1f} ç§’")
                time.sleep(rest_time)
        
        return articles
    
    def scrape_all_categories(self, urls: List[str], max_per_category: int = 10) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰é¡åˆ¥"""
        all_articles = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n\n{'='*70}")
            print(f"è™•ç†é¡åˆ¥ {i}/{len(urls)}")
            print(f"{'='*70}")
            
            articles = self.scrape_category(url, max_per_category)
            all_articles.extend(articles)
            
            # é¡åˆ¥ä¹‹é–“ä¼‘æ¯ 20-30 ç§’
            if i < len(urls):
                rest_time = random.uniform(20, 30)
                print(f"\nå®Œæˆæ­¤é¡åˆ¥ï¼Œä¼‘æ¯ {rest_time:.1f} ç§’å¾Œç¹¼çºŒ...")
                time.sleep(rest_time)
        
        return all_articles
    
    def translate_articles(self, articles: List[Dict]):
        """æ‰¹æ¬¡ç¿»è­¯æ–‡ç«  - æ”¹é€²ç‰ˆ"""
        print(f"\n{'='*70}")
        print("é–‹å§‹ç¿»è­¯æ–‡ç« ...")
        print(f"æç¤ºï¼šç¿»è­¯å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…")
        print(f"{'='*70}\n")
        
        successful = 0
        failed = 0
        
        for i, article in enumerate(articles, 1):
            print(f"\n{'='*70}")
            print(f"ç¿»è­¯é€²åº¦: {i}/{len(articles)}")
            print(f"{'='*70}")
            print(f"æ¨™é¡Œ: {article['title_ru'][:50]}...")
            
            # ç¿»è­¯æ¨™é¡Œ
            if article.get('title_ru'):
                article['title_zh'] = self.translate_to_chinese(article['title_ru'])
                if article['title_zh'] != "[ç¿»è­¯å¤±æ•—]":
                    print(f"âœ“ æ¨™é¡Œ: {article['title_zh'][:50]}...")
                    successful += 1
                else:
                    print(f"âœ— æ¨™é¡Œç¿»è­¯å¤±æ•—")
                    failed += 1
                time.sleep(random.uniform(2, 4))  # å¢åŠ å»¶é²é¿å…è¢«é™æµ
            else:
                article['title_zh'] = ''
            
            # ç¿»è­¯å…§æ–‡
            if article.get('content_ru'):
                print(f"ç¿»è­¯å…§æ–‡ ({len(article['content_ru'])} å­—ç¬¦)...")
                article['content_zh'] = self.translate_to_chinese(article['content_ru'])
                
                # æª¢æŸ¥æ˜¯å¦æˆåŠŸ
                if article['content_zh'] == "[ç¿»è­¯å¤±æ•—]":
                    print(f"âœ— å…§æ–‡ç¿»è­¯å¤±æ•—")
                else:
                    print(f"âœ“ å…§æ–‡ç¿»è­¯å®Œæˆ ({len(article['content_zh'])} å­—ç¬¦)")
                
                time.sleep(random.uniform(3, 6))  # æ›´é•·çš„å»¶é²
            else:
                article['content_zh'] = ''
            
            # æ¯5ç¯‡ä¼‘æ¯ä¸€ä¸‹
            if i % 5 == 0 and i < len(articles):
                rest_time = random.uniform(15, 20)
                print(f"\n{'ğŸ”„'*20}")
                print(f"å·²ç¿»è­¯ {i}/{len(articles)} ç¯‡")
                print(f"ä¼‘æ¯ {rest_time:.1f} ç§’é¿å…é™æµ...")
                print(f"{'ğŸ”„'*20}")
                time.sleep(rest_time)
        
        print(f"\n{'='*70}")
        print(f"ç¿»è­¯çµ±è¨ˆ: æˆåŠŸ {successful}, å¤±æ•— {failed}")
        print(f"{'='*70}")
    
    def save_to_excel(self, articles: List[Dict], filename: str = 'tass_news.xlsx'):
        """å„²å­˜ç‚ºExcelä¸¦è¨­å®šè‡ªå‹•æ›è¡Œ"""
        data = []
        for article in articles:
            data.append({
                'æ—¥æœŸ': article.get('date', ''),
                'ä½œè€…': article.get('author', ''),
                'æ¨™é¡Œ_ä¿„æ–‡': article.get('title_ru', ''),
                'æ¨™é¡Œ_ä¸­æ–‡': article.get('title_zh', ''),
                'å…§æ–‡_ä¿„æ–‡': article.get('content_ru', ''),
                'å…§æ–‡_ä¸­æ–‡': article.get('content_zh', ''),
                'ç¶²å€': article.get('url', ''),
                'ä¸‹è¼‰æ™‚é–“': article.get('download_datetime', ''),
                'åˆ†é¡': article.get('category', ''),
            })
        
        df = pd.DataFrame(data)
        df.to_excel(filename, index=False, engine='openpyxl')
        
        # æ ¼å¼è¨­å®š
        wb = load_workbook(filename)
        ws = wb.active
        
        column_widths = {
            'A': 18, 'B': 20, 'C': 60, 'D': 60,
            'E': 100, 'F': 100, 'G': 50, 'H': 20, 'I': 15,
        }
        
        for col, width in column_widths.items():
            ws.column_dimensions[col].width = width
        
        # è¨­å®šè‡ªå‹•æ›è¡Œ
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=9):
            for cell in row:
                cell.alignment = Alignment(
                    wrap_text=True,
                    vertical='top',
                    horizontal='left'
                )
        
        # æ¨™é¡Œè¡Œ
        for cell in ws[1]:
            cell.alignment = Alignment(
                wrap_text=True,
                vertical='center',
                horizontal='center'
            )
        
        wb.save(filename)
        print(f"\nâœ“ å·²å„²å­˜Excelæª”æ¡ˆ: {filename}")
        print(f"  å·²è¨­å®šè‡ªå‹•æ›è¡Œ")


def main():
    """ä¸»ç¨‹å¼"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘       TASS æ–°èçˆ¬èŸ² (æ”¹é€²ç‰ˆ - é›™ç¿»è­¯å¼•æ“)               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    urls = [
        'https://tass.ru/politika',
        'https://tass.ru/ekonomika',
        'https://tass.ru/armiya-i-opk',
        'https://tass.ru/mezhdunarodnaya-panorama',
    ]
    
    # å»ºç«‹çˆ¬èŸ²ï¼ˆheadless=False å¯ä»¥çœ‹åˆ°ç€è¦½å™¨é‹ä½œï¼‰
    scraper = TASSNewsScraper(headless=True)
    
    # è¨­å®šæ¯å€‹é¡åˆ¥æŠ“å– 20 ç¯‡æ–‡ç« 
    max_per_category = 20
    
    print(f"\nå°‡å¾ {len(urls)} å€‹é¡åˆ¥å„æŠ“å–æœ€å¤š {max_per_category} ç¯‡æ–‡ç« ")
    print(f"é è¨ˆç¸½æ–‡ç« æ•¸: {len(urls) * max_per_category} ç¯‡")
    print(f"é ä¼°æ™‚é–“: çˆ¬å– 40-60 åˆ†é˜ + ç¿»è­¯ 60-100 åˆ†é˜ = ç¸½è¨ˆç´„ 100-160 åˆ†é˜\n")
    
    try:
        # çˆ¬å–æ‰€æœ‰é¡åˆ¥
        articles = scraper.scrape_all_categories(urls, max_per_category)
        
        if not articles:
            print("\nâŒ æœªçˆ¬å–åˆ°ä»»ä½•æ–‡ç« ï¼")
            return
        
        print(f"\nâœ“ æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
        
        # è‡ªå‹•é€²è¡Œç¿»è­¯ï¼ˆä¸è©¢å•ï¼‰
        print("\n" + "="*70)
        print("é–‹å§‹è‡ªå‹•ç¿»è­¯...")
        print("="*70)
        
        try:
            scraper.translate_articles(articles)
        except ImportError as e:
            print(f"\nâš ï¸ ç¼ºå°‘ç¿»è­¯å¥—ä»¶: {e}")
            print("è«‹åŸ·è¡Œ:")
            print("  pip install googletrans==4.0.0-rc1")
            print("  pip install deep-translator")
            # ç¿»è­¯å¤±æ•—æ™‚å¡«å…¥ç©ºå€¼
            for article in articles:
                article['title_zh'] = ''
                article['content_zh'] = ''
        except Exception as e:
            print(f"\nâš ï¸ ç¿»è­¯éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            # ç¢ºä¿æ‰€æœ‰æ–‡ç« éƒ½æœ‰ç¿»è­¯æ¬„ä½
            for article in articles:
                if 'title_zh' not in article:
                    article['title_zh'] = ''
                if 'content_zh' not in article:
                    article['content_zh'] = ''
        
        # å„²å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'tass_news_{timestamp}.xlsx'
        
        scraper.save_to_excel(articles, filename)
        
        print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                     çˆ¬å–å®Œæˆï¼                            â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  æ–‡ç« æ•¸é‡: {len(articles):>3} ç¯‡                                    â•‘
    â•‘  æª”æ¡ˆåç¨±: {filename:<40} â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        # çµ±è¨ˆ
        print("\nå„é¡åˆ¥çµ±è¨ˆ:")
        from collections import Counter
        category_count = Counter(article.get('category', 'æœªçŸ¥') for article in articles)
        for category, count in category_count.items():
            print(f"  {category}: {count} ç¯‡")
    
    finally:
        # ç¢ºä¿ç€è¦½å™¨é—œé–‰
        del scraper


if __name__ == '__main__':
    main()